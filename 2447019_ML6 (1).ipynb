{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p5e4F_xxiqps"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((3, 3))  # 3x3 board initialized with zeros\n",
        "        self.player = 1  # Player 1 starts the game\n",
        "\n",
        "    def available_moves(self):\n",
        "        \"\"\"Returns a list of available moves (positions) on the board.\"\"\"\n",
        "        return np.argwhere(self.board == 0)\n",
        "    def make_move(self, row, col):\n",
        "        \"\"\"Places the player's mark (1 or -1) on the board at the given position.\"\"\"\n",
        "        if self.board[row, col] == 0:\n",
        "            self.board[row, col] = self.player\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def switch_player(self):\n",
        "        \"\"\"Switches the current player (from 1 to -1, or vice versa).\"\"\"\n",
        "        self.player *= -1\n",
        "\n",
        "    def check_winner(self):\n",
        "        \"\"\"Checks if there's a winner or if the game is a draw.\"\"\"\n",
        "        for i in range(3):\n",
        "            if abs(sum(self.board[i, :])) == 3:  # Check rows\n",
        "                return self.board[i, 0]\n",
        "            if abs(sum(self.board[:, i])) == 3:  # Check columns\n",
        "                return self.board[0, i]\n",
        "         # Check diagonals\n",
        "        if abs(self.board.trace()) == 3:\n",
        "            return self.board[0, 0]\n",
        "        if abs(np.fliplr(self.board).trace()) == 3:\n",
        "            return self.board[0, 2]\n",
        "        # Check for draw\n",
        "        if not self.available_moves().size:\n",
        "            return 0  # Draw\n",
        "        return None  # Game ongoing"
      ],
      "metadata": {
        "id": "E9XxDD0ai4Ws"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Task 2: Defining the Reinforcement Learning Model\n",
        "class Agent:\n",
        "    def __init__(self, player, epsilon=0.1, alpha=0.5, gamma=0.9):\n",
        "        self.player = player  # 1 for player 1, -1 for player 2\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.q_table = {}  # State-action value table\n",
        "\n",
        "    def get_state(self, board):\n",
        "        \"\"\"Converts the board into a tuple to use as a state.\"\"\"\n",
        "        return tuple(map(tuple, board))\n",
        "\n",
        "    def choose_action(self, game):\n",
        "        \"\"\"Chooses an action based on epsilon-greedy policy.\"\"\"\n",
        "        state = self.get_state(game.board)\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            # Exploration: choose a random move\n",
        "            available_moves = game.available_moves()\n",
        "            move = random.choice(available_moves)\n",
        "        else:\n",
        "            # Exploitation: choose the best move\n",
        "            q_values = self.q_table.get(state, {})\n",
        "            if q_values:\n",
        "                max_q = max(q_values.values())\n",
        "                best_moves = [move for move, q in q_values.items() if q == max_q]\n",
        "                move = random.choice(best_moves)\n",
        "            else:\n",
        "                move = random.choice(game.available_moves())\n",
        "        return move\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        \"\"\"Updates the Q-value for the given state and action.\"\"\"\n",
        "        current_q = self.q_table.get(state, {}).get(action, 0)\n",
        "        max_future_q = max(self.q_table.get(next_state, {}).values(), default=0)\n",
        "        new_q = current_q + self.alpha * (reward + self.gamma * max_future_q - current_q)\n",
        "        if state not in self.q_table:\n",
        "            self.q_table[state] = {}\n",
        "        self.q_table[state][action] = new_q\n",
        "\n",
        "    def learn(self, game, reward):\n",
        "        \"\"\"Updates Q-values after the game is finished.\"\"\"\n",
        "        state = self.get_state(game.board)\n",
        "        available_moves = game.available_moves()\n",
        "        if available_moves.size:\n",
        "            action = tuple(random.choice(available_moves))\n",
        "            next_state = self.get_state(game.board)\n",
        "            self.update_q_value(state, action, reward, next_state)\n",
        "\n",
        "# Create agents for both players\n",
        "player1 = Agent(player=1)\n",
        "player2 = Agent(player=-1)\n",
        "\n",
        "# Task 3: Training the Model\n",
        "def play_game(player1, player2, episodes=10000):\n",
        "    for episode in range(episodes):\n",
        "        game = TicTacToe()\n",
        "        while True:\n",
        "            if game.player == 1:\n",
        "                action = player1.choose_action(game)\n",
        "            else:\n",
        "                action = player2.choose_action(game)\n",
        "            game.make_move(*action)\n",
        "\n",
        "            winner = game.check_winner()\n",
        "            if winner is not None:\n",
        "                # Update Q-values after the game ends\n",
        "                if winner == 1:\n",
        "                    player1.learn(game, reward=1)\n",
        "                    player2.learn(game, reward=-1)\n",
        "                elif winner == -1:\n",
        "                    player1.learn(game, reward=-1)\n",
        "                    player2.learn(game, reward=1)\n",
        "                else:\n",
        "                    player1.learn(game, reward=0)  # Draw\n",
        "                    player2.learn(game, reward=0)  # Draw\n",
        "                break\n",
        "            game.switch_player()\n",
        "\n",
        "# Train the agents\n",
        "play_game(player1, player2, episodes=10000)\n",
        "\n"
      ],
      "metadata": {
        "id": "qI7wGTPPjspH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "with open('tictactoe_q_learning.pkl', 'wb') as f:\n",
        "    pickle.dump((player1.q_table, player2.q_table), f)\n",
        "\n",
        "# Task 4: Testing the Model\n",
        "def test_agent(agent, episodes=100):\n",
        "    \"\"\"Test the performance of the trained agent against a random opponent.\"\"\"\n",
        "    wins, losses, draws = 0, 0, 0\n",
        "    for episode in range(episodes):\n",
        "        game = TicTacToe()\n",
        "        while True:\n",
        "            if game.player == 1:\n",
        "                action = agent.choose_action(game)\n",
        "            else:\n",
        "                action = random.choice(game.available_moves())\n",
        "            game.make_move(*action)\n",
        "\n",
        "            winner = game.check_winner()\n",
        "            if winner is not None:\n",
        "                if winner == 1:\n",
        "                    wins += 1\n",
        "                elif winner == -1:\n",
        "                    losses += 1\n",
        "                else:\n",
        "                    draws += 1\n",
        "                break\n",
        "            game.switch_player()\n",
        "\n",
        "    return wins, losses, draws\n",
        "\n",
        "# Test player1 agent (trained) against a random player\n",
        "test_results = test_agent(player1, episodes=100)\n",
        "test_results  # Display test results (wins, losses, draws)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDWWyNiSp1EM",
        "outputId": "3dfa6766-1646-44b5-d590-f8eb8b09d343"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54, 33, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}